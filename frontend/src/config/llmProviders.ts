import { ProviderConfig, LLMProvider } from '../types/llm.types';

export const LLM_PROVIDERS: Record<LLMProvider, ProviderConfig> = {
  openai: {
    id: 'openai',
    name: 'OpenAI',
    icon: 'ðŸ¤–',
    color: '#10A37F',
    bgColor: '#ECFDF5',
    apiKeyPlaceholder: 'sk-...',
    apiKeyPrefix: 'sk-',
    docsUrl: 'https://platform.openai.com/docs',
    models: [
      {
        id: 'gpt-4o',
        name: 'GPT-4o',
        provider: 'openai',
        contextWindow: 128000,
        maxOutput: 4096,
        inputPricing: 5,
        outputPricing: 15,
        capabilities: ['chat', 'vision', 'function_calling', 'json_mode', 'streaming'],
        description: 'Most capable model, great for complex tasks',
        isNew: true,
      },
      {
        id: 'gpt-4o-mini',
        name: 'GPT-4o Mini',
        provider: 'openai',
        contextWindow: 128000,
        maxOutput: 16384,
        inputPricing: 0.15,
        outputPricing: 0.6,
        capabilities: ['chat', 'vision', 'function_calling', 'json_mode', 'streaming'],
        description: 'Fast and affordable, great for most tasks',
      },
      {
        id: 'gpt-4-turbo',
        name: 'GPT-4 Turbo',
        provider: 'openai',
        contextWindow: 128000,
        maxOutput: 4096,
        inputPricing: 10,
        outputPricing: 30,
        capabilities: ['chat', 'vision', 'function_calling', 'json_mode', 'streaming'],
        description: 'Previous flagship with vision capabilities',
      },
      {
        id: 'gpt-4',
        name: 'GPT-4',
        provider: 'openai',
        contextWindow: 8192,
        maxOutput: 4096,
        inputPricing: 30,
        outputPricing: 60,
        capabilities: ['chat', 'function_calling', 'streaming'],
        description: 'Original GPT-4 model',
      },
      {
        id: 'gpt-3.5-turbo',
        name: 'GPT-3.5 Turbo',
        provider: 'openai',
        contextWindow: 16385,
        maxOutput: 4096,
        inputPricing: 0.5,
        outputPricing: 1.5,
        capabilities: ['chat', 'function_calling', 'json_mode', 'streaming'],
        description: 'Fast and cost-effective',
      },
    ],
  },

  google: {
    id: 'google',
    name: 'Google AI',
    icon: 'ðŸ”·',
    color: '#4285F4',
    bgColor: '#EBF5FF',
    apiKeyPlaceholder: 'AIza...',
    apiKeyPrefix: 'AIza',
    docsUrl: 'https://ai.google.dev/docs',
    models: [
      {
        id: 'gemini-2.0-flash-exp',
        name: 'Gemini 2.0 Flash',
        provider: 'google',
        contextWindow: 1000000,
        maxOutput: 8192,
        capabilities: ['chat', 'vision', 'function_calling', 'streaming'],
        description: 'Latest Gemini model, multimodal capabilities',
        isNew: true,
      },
      {
        id: 'gemini-1.5-pro',
        name: 'Gemini 1.5 Pro',
        provider: 'google',
        contextWindow: 2000000,
        maxOutput: 8192,
        inputPricing: 1.25,
        outputPricing: 5,
        capabilities: ['chat', 'vision', 'function_calling', 'json_mode', 'streaming'],
        description: 'Best for complex reasoning, 2M context',
      },
      {
        id: 'gemini-1.5-flash',
        name: 'Gemini 1.5 Flash',
        provider: 'google',
        contextWindow: 1000000,
        maxOutput: 8192,
        inputPricing: 0.075,
        outputPricing: 0.3,
        capabilities: ['chat', 'vision', 'function_calling', 'json_mode', 'streaming'],
        description: 'Fast and efficient, 1M context',
      },
      {
        id: 'gemini-1.5-flash-8b',
        name: 'Gemini 1.5 Flash 8B',
        provider: 'google',
        contextWindow: 1000000,
        maxOutput: 8192,
        inputPricing: 0.0375,
        outputPricing: 0.15,
        capabilities: ['chat', 'vision', 'function_calling', 'streaming'],
        description: 'Smallest and fastest Gemini model',
      },
    ],
  },

  anthropic: {
    id: 'anthropic',
    name: 'Anthropic',
    icon: 'ðŸ…°ï¸',
    color: '#D97706',
    bgColor: '#FFFBEB',
    apiKeyPlaceholder: 'sk-ant-...',
    apiKeyPrefix: 'sk-ant-',
    docsUrl: 'https://docs.anthropic.com',
    models: [
      {
        id: 'claude-3-5-sonnet-20241022',
        name: 'Claude 3.5 Sonnet',
        provider: 'anthropic',
        contextWindow: 200000,
        maxOutput: 8192,
        inputPricing: 3,
        outputPricing: 15,
        capabilities: ['chat', 'vision', 'function_calling', 'streaming'],
        description: 'Most intelligent Claude model',
        isNew: true,
      },
      {
        id: 'claude-3-5-haiku-20241022',
        name: 'Claude 3.5 Haiku',
        provider: 'anthropic',
        contextWindow: 200000,
        maxOutput: 8192,
        inputPricing: 0.8,
        outputPricing: 4,
        capabilities: ['chat', 'vision', 'function_calling', 'streaming'],
        description: 'Fast and affordable Claude model',
        isNew: true,
      },
      {
        id: 'claude-3-opus-20240229',
        name: 'Claude 3 Opus',
        provider: 'anthropic',
        contextWindow: 200000,
        maxOutput: 4096,
        inputPricing: 15,
        outputPricing: 75,
        capabilities: ['chat', 'vision', 'function_calling', 'streaming'],
        description: 'Most powerful for complex tasks',
      },
      {
        id: 'claude-3-sonnet-20240229',
        name: 'Claude 3 Sonnet',
        provider: 'anthropic',
        contextWindow: 200000,
        maxOutput: 4096,
        inputPricing: 3,
        outputPricing: 15,
        capabilities: ['chat', 'vision', 'function_calling', 'streaming'],
        description: 'Balanced performance and speed',
      },
      {
        id: 'claude-3-haiku-20240307',
        name: 'Claude 3 Haiku',
        provider: 'anthropic',
        contextWindow: 200000,
        maxOutput: 4096,
        inputPricing: 0.25,
        outputPricing: 1.25,
        capabilities: ['chat', 'vision', 'function_calling', 'streaming'],
        description: 'Fastest Claude model',
      },
    ],
  },

  xai: {
    id: 'xai',
    name: 'xAI',
    icon: 'âœ–ï¸',
    color: '#000000',
    bgColor: '#F3F4F6',
    apiKeyPlaceholder: 'xai-...',
    apiKeyPrefix: 'xai-',
    docsUrl: 'https://docs.x.ai',
    models: [
      {
        id: 'grok-2-1212',
        name: 'Grok 2',
        provider: 'xai',
        contextWindow: 131072,
        maxOutput: 4096,
        inputPricing: 2,
        outputPricing: 10,
        capabilities: ['chat', 'function_calling', 'streaming'],
        description: 'Latest Grok model with improved reasoning',
        isNew: true,
      },
      {
        id: 'grok-2-vision-1212',
        name: 'Grok 2 Vision',
        provider: 'xai',
        contextWindow: 32768,
        maxOutput: 4096,
        inputPricing: 2,
        outputPricing: 10,
        capabilities: ['chat', 'vision', 'function_calling', 'streaming'],
        description: 'Grok with vision capabilities',
        isNew: true,
      },
      {
        id: 'grok-beta',
        name: 'Grok Beta',
        provider: 'xai',
        contextWindow: 131072,
        maxOutput: 4096,
        inputPricing: 5,
        outputPricing: 15,
        capabilities: ['chat', 'function_calling', 'streaming'],
        description: 'Beta version of Grok',
        isBeta: true,
      },
    ],
  },

  meta: {
    id: 'meta',
    name: 'Meta Llama',
    icon: 'ðŸ¦™',
    color: '#0668E1',
    bgColor: '#EFF6FF',
    apiKeyPlaceholder: 'Use via Groq, Together, or local',
    docsUrl: 'https://llama.meta.com',
    models: [
      {
        id: 'llama-3.3-70b-versatile',
        name: 'Llama 3.3 70B',
        provider: 'meta',
        contextWindow: 128000,
        maxOutput: 32768,
        capabilities: ['chat', 'function_calling', 'streaming'],
        description: 'Latest Llama model, highly capable',
        isNew: true,
      },
      {
        id: 'llama-3.2-90b-vision-preview',
        name: 'Llama 3.2 90B Vision',
        provider: 'meta',
        contextWindow: 128000,
        maxOutput: 8192,
        capabilities: ['chat', 'vision', 'streaming'],
        description: 'Multimodal Llama with vision',
        isNew: true,
      },
      {
        id: 'llama-3.1-70b-versatile',
        name: 'Llama 3.1 70B',
        provider: 'meta',
        contextWindow: 128000,
        maxOutput: 8192,
        capabilities: ['chat', 'function_calling', 'streaming'],
        description: 'Powerful open-source model',
      },
      {
        id: 'llama-3.1-8b-instant',
        name: 'Llama 3.1 8B',
        provider: 'meta',
        contextWindow: 128000,
        maxOutput: 8192,
        capabilities: ['chat', 'streaming'],
        description: 'Fast and efficient',
      },
      {
        id: 'llama-guard-3-8b',
        name: 'Llama Guard 3 8B',
        provider: 'meta',
        contextWindow: 8192,
        maxOutput: 4096,
        capabilities: ['chat'],
        description: 'Safety-focused model for content moderation',
      },
    ],
  },

  mistral: {
    id: 'mistral',
    name: 'Mistral AI',
    icon: 'ðŸŒ€',
    color: '#FF7000',
    bgColor: '#FFF7ED',
    apiKeyPlaceholder: 'Enter Mistral API key',
    docsUrl: 'https://docs.mistral.ai',
    models: [
      {
        id: 'mistral-large-latest',
        name: 'Mistral Large',
        provider: 'mistral',
        contextWindow: 128000,
        maxOutput: 4096,
        inputPricing: 2,
        outputPricing: 6,
        capabilities: ['chat', 'function_calling', 'json_mode', 'streaming'],
        description: 'Most capable Mistral model',
        isNew: true,
      },
      {
        id: 'mistral-small-latest',
        name: 'Mistral Small',
        provider: 'mistral',
        contextWindow: 32000,
        maxOutput: 4096,
        inputPricing: 0.2,
        outputPricing: 0.6,
        capabilities: ['chat', 'function_calling', 'json_mode', 'streaming'],
        description: 'Cost-effective for simple tasks',
      },
      {
        id: 'codestral-latest',
        name: 'Codestral',
        provider: 'mistral',
        contextWindow: 32000,
        maxOutput: 4096,
        inputPricing: 0.2,
        outputPricing: 0.6,
        capabilities: ['chat', 'completion', 'streaming'],
        description: 'Optimized for code generation',
      },
      {
        id: 'ministral-8b-latest',
        name: 'Ministral 8B',
        provider: 'mistral',
        contextWindow: 128000,
        maxOutput: 4096,
        inputPricing: 0.1,
        outputPricing: 0.1,
        capabilities: ['chat', 'streaming'],
        description: 'Smallest and fastest Mistral model',
      },
      {
        id: 'pixtral-12b-2409',
        name: 'Pixtral 12B',
        provider: 'mistral',
        contextWindow: 128000,
        maxOutput: 4096,
        inputPricing: 0.15,
        outputPricing: 0.15,
        capabilities: ['chat', 'vision', 'streaming'],
        description: 'Multimodal model with vision',
      },
    ],
  },

  groq: {
    id: 'groq',
    name: 'Groq',
    icon: 'âš¡',
    color: '#F55036',
    bgColor: '#FEF2F2',
    apiKeyPlaceholder: 'gsk_...',
    apiKeyPrefix: 'gsk_',
    docsUrl: 'https://console.groq.com/docs',
    models: [
      {
        id: 'llama-3.3-70b-versatile',
        name: 'Llama 3.3 70B (Groq)',
        provider: 'groq',
        contextWindow: 128000,
        maxOutput: 32768,
        inputPricing: 0.59,
        outputPricing: 0.79,
        capabilities: ['chat', 'function_calling', 'streaming'],
        description: 'Llama 3.3 on Groq - blazing fast',
        isNew: true,
      },
      {
        id: 'llama-3.1-70b-versatile',
        name: 'Llama 3.1 70B (Groq)',
        provider: 'groq',
        contextWindow: 128000,
        maxOutput: 8192,
        inputPricing: 0.59,
        outputPricing: 0.79,
        capabilities: ['chat', 'function_calling', 'streaming'],
        description: 'Fast inference on Groq hardware',
      },
      {
        id: 'llama-3.1-8b-instant',
        name: 'Llama 3.1 8B (Groq)',
        provider: 'groq',
        contextWindow: 128000,
        maxOutput: 8192,
        inputPricing: 0.05,
        outputPricing: 0.08,
        capabilities: ['chat', 'streaming'],
        description: 'Ultra-fast small model',
      },
      {
        id: 'mixtral-8x7b-32768',
        name: 'Mixtral 8x7B (Groq)',
        provider: 'groq',
        contextWindow: 32768,
        maxOutput: 4096,
        inputPricing: 0.24,
        outputPricing: 0.24,
        capabilities: ['chat', 'function_calling', 'streaming'],
        description: 'Mixture of experts model',
      },
      {
        id: 'gemma2-9b-it',
        name: 'Gemma 2 9B (Groq)',
        provider: 'groq',
        contextWindow: 8192,
        maxOutput: 4096,
        inputPricing: 0.2,
        outputPricing: 0.2,
        capabilities: ['chat', 'streaming'],
        description: 'Google Gemma on Groq',
      },
    ],
  },

  cohere: {
    id: 'cohere',
    name: 'Cohere',
    icon: 'ðŸ”®',
    color: '#39594D',
    bgColor: '#ECFDF5',
    apiKeyPlaceholder: 'Enter Cohere API key',
    docsUrl: 'https://docs.cohere.com',
    models: [
      {
        id: 'command-r-plus',
        name: 'Command R+',
        provider: 'cohere',
        contextWindow: 128000,
        maxOutput: 4096,
        inputPricing: 2.5,
        outputPricing: 10,
        capabilities: ['chat', 'function_calling', 'streaming'],
        description: 'Most capable Cohere model',
      },
      {
        id: 'command-r',
        name: 'Command R',
        provider: 'cohere',
        contextWindow: 128000,
        maxOutput: 4096,
        inputPricing: 0.15,
        outputPricing: 0.6,
        capabilities: ['chat', 'function_calling', 'streaming'],
        description: 'Balanced performance and cost',
      },
      {
        id: 'command-light',
        name: 'Command Light',
        provider: 'cohere',
        contextWindow: 4096,
        maxOutput: 4096,
        inputPricing: 0.3,
        outputPricing: 0.6,
        capabilities: ['chat', 'streaming'],
        description: 'Fast and lightweight',
      },
    ],
  },
};

// Helper functions
export const getProviderById = (id: LLMProvider): ProviderConfig | undefined => {
  return LLM_PROVIDERS[id];
};

export const getModelById = (providerId: LLMProvider, modelId: string) => {
  const provider = LLM_PROVIDERS[providerId];
  return provider?.models.find(m => m.id === modelId);
};

export const getAllProviders = (): ProviderConfig[] => {
  return Object.values(LLM_PROVIDERS);
};

export const getProviderModels = (providerId: LLMProvider) => {
  return LLM_PROVIDERS[providerId]?.models || [];
};

export const getDefaultModel = (providerId: LLMProvider): string => {
  const provider = LLM_PROVIDERS[providerId];
  if (!provider) return '';
  
  // Return the first model marked as new, or the first model
  const newModel = provider.models.find(m => m.isNew);
  return newModel?.id || provider.models[0]?.id || '';
};

// Provider order for display
export const PROVIDER_ORDER: LLMProvider[] = [
  'openai',
  'google',
  'anthropic',
  'xai',
  'groq',
  'mistral',
  'meta',
  'cohere',
];