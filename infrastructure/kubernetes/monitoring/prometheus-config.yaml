apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: monitoring
  labels:
    app: prometheus
    app.kubernetes.io/name: prometheus
    app.kubernetes.io/part-of: askyia
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s
      external_labels:
        cluster: 'askyia-cluster'
        environment: 'production'

    alerting:
      alertmanagers:
        - static_configs:
            - targets: []  # Add alertmanager if deployed

    rule_files:
      - /etc/prometheus/rules/*.yml

    scrape_configs:
      # Prometheus self-monitoring
      - job_name: 'prometheus'
        static_configs:
          - targets: ['localhost:9090']
        scrape_interval: 10s

      # Askyia Backend API (cross-namespace)
      - job_name: 'askyia-backend'
        static_configs:
          - targets: ['backend.askyia.svc.cluster.local:8000']
        metrics_path: /metrics
        scrape_interval: 10s

      # Kubernetes API server
      - job_name: 'kubernetes-apiservers'
        kubernetes_sd_configs:
          - role: endpoints
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        relabel_configs:
          - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
            action: keep
            regex: default;kubernetes;https

      # Kubernetes nodes
      - job_name: 'kubernetes-nodes'
        kubernetes_sd_configs:
          - role: node
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        relabel_configs:
          - action: labelmap
            regex: __meta_kubernetes_node_label_(.+)

      # Kubernetes pods with prometheus annotations
      - job_name: 'kubernetes-pods'
        kubernetes_sd_configs:
          - role: pod
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
            action: replace
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
            target_label: __address__
          - action: labelmap
            regex: __meta_kubernetes_pod_label_(.+)
          - source_labels: [__meta_kubernetes_namespace]
            action: replace
            target_label: kubernetes_namespace
          - source_labels: [__meta_kubernetes_pod_name]
            action: replace
            target_label: kubernetes_pod_name

      # Kubernetes services
      - job_name: 'kubernetes-services'
        kubernetes_sd_configs:
          - role: service
        metrics_path: /probe
        params:
          module: [http_2xx]
        relabel_configs:
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]
            action: keep
            regex: true
          - source_labels: [__address__]
            target_label: __param_target
          - target_label: __address__
            replacement: blackbox-exporter:9115
          - source_labels: [__param_target]
            target_label: instance
          - action: labelmap
            regex: __meta_kubernetes_service_label_(.+)
          - source_labels: [__meta_kubernetes_namespace]
            target_label: kubernetes_namespace
          - source_labels: [__meta_kubernetes_service_name]
            target_label: kubernetes_name

      # Elasticsearch metrics (cross-namespace)
      - job_name: 'elasticsearch'
        static_configs:
          - targets: ['elasticsearch.logging.svc.cluster.local:9200']
        metrics_path: /_prometheus/metrics
        scrape_interval: 30s

  # Alert rules
  askyia-rules.yml: |
    groups:
      - name: askyia-api-alerts
        rules:
          # High error rate alert
          - alert: HighErrorRate
            expr: |
              (
                sum(rate(http_requests_total{status_code=~"5.."}[5m]))
                / sum(rate(http_requests_total[5m]))
              ) > 0.05
            for: 5m
            labels:
              severity: critical
              service: askyia-backend
            annotations:
              summary: "High HTTP error rate detected"
              description: "Error rate is {{ $value | humanizePercentage }} over the last 5 minutes"

          # High latency alert
          - alert: HighLatency
            expr: |
              histogram_quantile(0.95,
                sum(rate(http_request_duration_seconds_bucket[5m])) by (le)
              ) > 2
            for: 5m
            labels:
              severity: warning
              service: askyia-backend
            annotations:
              summary: "High API latency detected"
              description: "95th percentile latency is {{ $value }}s"

          # API down alert
          - alert: APIDown
            expr: up{job="askyia-backend"} == 0
            for: 1m
            labels:
              severity: critical
              service: askyia-backend
            annotations:
              summary: "Askyia Backend API is down"
              description: "The backend API has been unreachable for more than 1 minute"

      - name: askyia-workflow-alerts
        rules:
          # High workflow failure rate
          - alert: WorkflowExecutionFailures
            expr: |
              (
                sum(rate(workflow_executions_total{status="error"}[5m]))
                / sum(rate(workflow_executions_total[5m]))
              ) > 0.1
            for: 5m
            labels:
              severity: warning
              service: askyia-backend
            annotations:
              summary: "High workflow execution failure rate"
              description: "{{ $value | humanizePercentage }} of workflows are failing"

          # Too many active workflows
          - alert: TooManyActiveWorkflows
            expr: active_workflows > 50
            for: 5m
            labels:
              severity: warning
              service: askyia-backend
            annotations:
              summary: "Too many concurrent workflow executions"
              description: "{{ $value }} workflows are currently executing"

          # Slow workflow execution
          - alert: SlowWorkflowExecution
            expr: |
              histogram_quantile(0.95,
                sum(rate(workflow_execution_duration_seconds_bucket[5m])) by (le)
              ) > 60
            for: 10m
            labels:
              severity: warning
              service: askyia-backend
            annotations:
              summary: "Workflow executions are slow"
              description: "95th percentile workflow duration is {{ $value }}s"

      - name: askyia-llm-alerts
        rules:
          # LLM API errors
          - alert: LLMAPIErrors
            expr: sum(rate(llm_requests_total{status="error"}[5m])) > 0.5
            for: 5m
            labels:
              severity: warning
              service: askyia-backend
            annotations:
              summary: "LLM API errors detected"
              description: "{{ $value }} LLM API errors per second"

          # High LLM latency
          - alert: HighLLMLatency
            expr: |
              histogram_quantile(0.95,
                sum(rate(llm_request_duration_seconds_bucket[5m])) by (le, provider)
              ) > 30
            for: 5m
            labels:
              severity: warning
              service: askyia-backend
            annotations:
              summary: "High LLM API latency"
              description: "95th percentile LLM latency is {{ $value }}s for {{ $labels.provider }}"

      - name: askyia-infrastructure-alerts
        rules:
          # PostgreSQL down
          - alert: PostgreSQLDown
            expr: pg_up == 0
            for: 1m
            labels:
              severity: critical
              service: postgres
            annotations:
              summary: "PostgreSQL is down"
              description: "PostgreSQL has been unreachable for more than 1 minute"

          # Elasticsearch down
          - alert: ElasticsearchDown
            expr: up{job="elasticsearch"} == 0
            for: 2m
            labels:
              severity: critical
              service: elasticsearch
            annotations:
              summary: "Elasticsearch is down"
              description: "Elasticsearch has been unreachable for more than 2 minutes"

          # ChromaDB down
          - alert: ChromaDBDown
            expr: up{job="chromadb"} == 0
            for: 2m
            labels:
              severity: critical
              service: chromadb
            annotations:
              summary: "ChromaDB is down"
              description: "ChromaDB has been unreachable for more than 2 minutes"

      - name: kubernetes-alerts
        rules:
          # Pod crash looping
          - alert: PodCrashLooping
            expr: |
              rate(kube_pod_container_status_restarts_total[15m]) * 60 * 15 > 0
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Pod is crash looping"
              description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is restarting frequently"

          # Pod not ready
          - alert: PodNotReady
            expr: |
              kube_pod_status_ready{condition="true"} == 0
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Pod not ready"
              description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is not ready"